<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<title>AG-EgoPose: Action-Guided Egocentric 3D Human Pose Estimation</title>
<link href="assets/css/style.css" rel="stylesheet"/>
</head>
<body>
<header class="hero" id="top">
<div class="logo-row">
<img alt="UTSA" class="logo" src="assets/img/utsa_logo.png"/>
<img alt="VIRLab" class="logo" src="assets/img/virlab_logo.png"/>
</div>
<div class="hero-inner">
<div class="badge">Under Review</div>
<h1 class="title">AG-EgoPose: Action-Guided Egocentric 3D Human Pose Estimation</h1>
<div class="authors">Md Mushfiqur Azam  ¬† Kevin Desai</div>
<div class="affil">University of Texas at San Antonio (UTSA), USA</div>

</div>
</header>
<main class="container">
<section class="section" id="abstract">
<h2>Abstract</h2>
<p class="lead">Egocentric 3D human pose estimation remains challenging due to severe perspective distortion, limited body visibility, and complex camera motion inherent in first-person viewpoints. Existing methods typically rely on single-frame analysis or limited temporal fusion, which fails to effectively leverage the rich motion context available in egocentric videos. We introduce <strong>AG-EgoPose</strong>, a dual-stream framework that integrates short- and long-range motion context with fine-grained spatial cues for robust pose estimation from fisheye camera input. Our framework features two parallel streams: a spatial stream uses a weight-sharing ResNet-18 encoder‚Äìdecoder to generate 2D joint heatmaps and corresponding joint-specific spatial feature tokens, while a temporal stream uses a ResNet-50 backbone with an action-recognition module to capture motion dynamics. These complementary representations are fused in a transformer decoder with learnable joint tokens, enabling joint-level integration of spatial and temporal evidence. Experiments on real-world datasets demonstrate strong performance in both quantitative and qualitative evaluations.</p>
<div class="callout">
<h3>Key Contributions</h3>
<ul class="checklist">
<li>Dual-stream design that jointly models fine-grained spatial cues (heatmaps/tokens) and long-term motion dynamics for egocentric pose.</li><li>Action-guided temporal stream that leverages an action recognition backbone to encode motion context beyond short clips.</li><li>Transformer decoder fusion with learnable joint tokens for joint-wise integration of spatial and temporal evidence.</li><li>Training with multi-term supervision (3D pose, heatmaps, and kinematic consistency) to encourage anatomically plausible predictions.</li>
</ul>
</div>
</section>
<section class="section" id="architecture">
<h2>Model Architecture</h2>
<figure class="figure">
<img alt="Model architecture figure" src="assets/img/architecture.png"/>
<figcaption>Overall AG-EgoPose architecture (spatial stream, temporal stream, and transformer fusion).</figcaption>
</figure>


</section>
<section class="section" id="method">
<h2>Method</h2>


<div class="method-grid">
<div class="method-card">
<div class="method-card-title"><span class="method-icon">üñºÔ∏è</span><span>Spatial Stream (Pose Cues)</span></div>
<p>Encodes each egocentric frame with a lightweight CNN to extract fine-grained spatial features that correlate with body configuration.</p>
</div>
<div class="method-card">
<div class="method-card-title"><span class="method-icon">üé¨</span><span>Temporal Stream (Action Dynamics)</span></div>
<p>Models longer-term motion context via an action-aware backbone, providing temporally coherent cues for pose disambiguation.</p>
</div>
<div class="method-card">
<div class="method-card-title"><span class="method-icon">üîÄ</span><span>Transformer Fusion</span></div>
<p>Fuses spatial and temporal representations using a transformer decoder with learnable joint tokens for joint-wise integration.</p>
</div>
<div class="method-card">
<div class="method-card-title"><span class="method-icon">ü¶¥</span><span>3D Pose Regression</span></div>
<p>Predicts per-frame 3D joints from fused tokens, optimized with standard 3D pose objectives on egocentric benchmarks.</p>
</div>
</div>
</section>
<section class="section" id="links">
<h2>Resources</h2>
<div class="button-row">
<a class="btn" href="assets/dummy.pdf" rel="noopener" target="_blank">Paper</a>
<a class="btn" href="assets/dummy.pdf" rel="noopener" target="_blank">arXiv (PDF)</a>
<a class="btn" href="https://edmond.mpg.de/dataset.xhtml?persistentId=doi:10.17617/3.FQAEOV" rel="noopener" target="_blank">EgoPW Dataset</a>
<a class="btn" href="https://edmond.mpg.de/dataset.xhtml?persistentId=doi:10.17617/3.VCIHDO" rel="noopener" target="_blank">SceneEgo Dataset</a>
<a class="btn" href="https://github.com/Mushfiq5647/AG-EgoPose" rel="noopener" target="_blank">Code</a>
</div>

</section>
<section class="section" id="bibtex">
<h2>BibTeX</h2>
<pre class="code"><code>@inproceedings{azam_agegopose_2026,
  title     = {AG-EgoPose: Action-Guided Egocentric 3D Human Pose Estimation},
  author    = {Azam, Md Mushfiqur and Desai, Kevin},
  booktitle = {Under Review},
  year      = {2026}
}</code></pre>
</section>
<section class="section" id="ack">
<h2>Acknowledgement</h2>
<p>This material is partially supported by the National Science Foundation under Grants 2316240 and 2403411.</p>
</section>
<footer class="footer">
<a href="index.html">Back to top</a>
</footer>
</main>
<script src="assets/js/main.js"></script>
</body>
</html>
